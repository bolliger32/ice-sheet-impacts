{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a8859a-9f58-48ab-b310-f03ca6de1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138ab805-3755-48c4-b1f5-141f78cf1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pyCIAM.io import get_nearest_slrs\n",
    "from shared import (\n",
    "    DIR_SLR_AR6_RAW,\n",
    "    PATH_SLIIDERS,\n",
    "    PATH_SLR_INT,\n",
    "    PATH_SLR_IS_RAW,\n",
    "    open_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea72758-ac27-49ea-9cc1-11a1ca4e9bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp126_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp245_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp585_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp119_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp126_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp245_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp370_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/total_ssp585_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp126_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp245_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp585_low_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp119_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp126_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp245_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp370_medium_confidence_values.nc...\n",
      "Processing gs://rhg-data/impactlab-rhg/coastal/ciam_paper/data/raw/slr/ar6/regional/verticallandmotion_ssp585_medium_confidence_values.nc...\n"
     ]
    }
   ],
   "source": [
    "all_ds = []\n",
    "global_ds = []\n",
    "global_ssps = []\n",
    "\n",
    "\n",
    "def open_and_convert(ds_path):\n",
    "    out = open_dataset(ds_path)\n",
    "    out[\"sea_level_change\"] = (\n",
    "        out.sea_level_change.pint.quantify().pint.to(\"meters\").pint.dequantify()\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "for kind in [\"total\", \"verticallandmotion\"]:\n",
    "    ds = []\n",
    "    this_ssps = []\n",
    "    for conf in [\"low\", \"medium\"]:\n",
    "        for ds_path in (DIR_SLR_AR6_RAW / \"regional\").glob(f\"{kind}_*_{conf}_*\"):\n",
    "            print(f\"Processing {ds_path}...\")\n",
    "            this_ssp = ds_path.name.split(\"_\")[1]\n",
    "            ssp_conf = f\"{this_ssp}_{conf}\"\n",
    "            ds.append(open_and_convert(ds_path))\n",
    "            this_ssps.append(ssp_conf)\n",
    "            if kind == \"total\":\n",
    "                global_ds.append(\n",
    "                    open_and_convert(DIR_SLR_AR6_RAW / \"global\" / ds_path.name)\n",
    "                )\n",
    "                global_ssps.append(ssp_conf)\n",
    "    all_ds.append(\n",
    "        xr.concat(ds, pd.Index(this_ssps, name=\"scenario\"), data_vars=\"different\")\n",
    "    )\n",
    "\n",
    "# for some reason the VLM dataset has an entry for 2005 that is all 0s, while other\n",
    "# datasets just don't have 2005 b/c it is the assumed basline\n",
    "assert (all_ds[1].sea_level_change.sel(years=2005) == 0).all()\n",
    "all_ds[1] = all_ds[1].sel(years=slice(2006, None))\n",
    "\n",
    "global_ds = (\n",
    "    xr.concat(global_ds, pd.Index(global_ssps, name=\"scenario\"), data_vars=\"different\")\n",
    "    .squeeze(drop=True)\n",
    "    .drop_vars([\"lon\", \"lat\"])\n",
    "    .sea_level_change\n",
    ")\n",
    "\n",
    "# handle floating point matching errors on the quantile dimension\n",
    "global_ds[\"quantiles\"] = all_ds[0].quantiles\n",
    "all_ds[1][\"quantiles\"] = all_ds[0].quantiles\n",
    "\n",
    "all_ds = xr.Dataset(\n",
    "    {\n",
    "        \"lsl_msl05\": all_ds[0].sea_level_change,\n",
    "        \"lsl_ncc_msl05\": all_ds[1].sea_level_change,\n",
    "        \"gsl_msl05\": global_ds,\n",
    "        \"lon\": all_ds[1].lon,\n",
    "        \"lat\": all_ds[0].lat,\n",
    "    }\n",
    ")\n",
    "\n",
    "# drop locations with NaN values in the time period we're interested in\n",
    "valid = (\n",
    "    all_ds[[\"lsl_msl05\", \"lsl_ncc_msl05\"]]\n",
    "    .sel(years=slice(2100))\n",
    "    .notnull()\n",
    "    .all([\"scenario\", \"quantiles\", \"years\"])\n",
    "    .to_array(\"tmp\")\n",
    "    .all(\"tmp\")\n",
    ")\n",
    "all_ds = all_ds.sel(locations=valid)\n",
    "\n",
    "all_ds = all_ds.rename(\n",
    "    {\"years\": \"year\", \"quantiles\": \"quantile\", \"locations\": \"site_id\"}\n",
    ")\n",
    "\n",
    "# we generally allow +180 but not -180\n",
    "all_ds[\"lon\"] = all_ds.lon.where(all_ds.lon != -180, 180)\n",
    "\n",
    "# ensure no locations have missing values\n",
    "assert all_ds.sel(year=slice(2100)).notnull().all().to_array().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6751079-9920-4c9a-8013-7d58da1bb7ba",
   "metadata": {},
   "source": [
    "## Add on the pulse from greenland and antarctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef494eb-75ed-4d52-b99d-18cc67cd3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliiders = (\n",
    "    xr.open_zarr(str(PATH_SLIIDERS))\n",
    "    .load()\n",
    "    .swap_dims(seg_adm=\"seg\")\n",
    "    .drop_duplicates(\"seg\")\n",
    ")\n",
    "\n",
    "slrs = get_nearest_slrs(\n",
    "    all_ds,\n",
    "    sliiders[[\"seg_lon\", \"seg_lat\"]].reset_coords(drop=True).to_dataframe(),\n",
    ")\n",
    "\n",
    "adders = (\n",
    "    pd.read_parquet(PATH_SLR_IS_RAW)\n",
    "    .loc[sliiders.seg.to_series().index]\n",
    "    .assign(constant=0.01)\n",
    "    .to_xarray()\n",
    "    .to_array(\"sheet\")\n",
    ")\n",
    "\n",
    "seg_ds = all_ds.sel(site_id=slrs.to_xarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea0c2701-083b-4fca-961c-5a939b073998",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsl = (seg_ds.lsl_msl05 + adders).rename(scenario=\"tmp\").stack(tmp2=[\"tmp\", \"sheet\"])\n",
    "lsl[\"scenario\"] = lsl.tmp + \"_\" + lsl.sheet\n",
    "lsl = lsl.set_index(tmp2=\"scenario\").reset_coords(drop=True).rename(tmp2=\"scenario\")\n",
    "\n",
    "gsl = xr.ones_like(lsl.isel(seg=0, drop=True)) / 100\n",
    "\n",
    "new_scens_added = xr.concat(\n",
    "    (\n",
    "        seg_ds[[\"lsl_msl05\", \"gsl_msl05\"]],\n",
    "        xr.Dataset({\"lsl_msl05\": lsl, \"gsl_msl05\": gsl}),\n",
    "    ),\n",
    "    dim=\"scenario\",\n",
    ")\n",
    "\n",
    "out = (\n",
    "    xr.merge((seg_ds.drop_dims(\"scenario\"), new_scens_added))\n",
    "    .swap_dims(seg=\"site_id\")\n",
    "    .drop_vars(\"seg\")\n",
    "    .drop_duplicates(\"site_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a7c34-ee76-41f5-9e4d-6593201ff96a",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba5b3f7-23ac-4857-9ce0-f74558cc653b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7837c0fe5a40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for v in out.variables:\n",
    "    out[v].encoding.clear()\n",
    "\n",
    "out.chunk({\"site_id\": 100}).to_zarr(str(PATH_SLR_INT), mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
