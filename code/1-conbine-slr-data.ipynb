{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138ab805-3755-48c4-b1f5-141f78cf1bcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pyCIAM.io import get_nearest_slrs\n",
    "from shared import (\n",
    "    DIR_SLR_AR6_RAW,\n",
    "    PATH_PARAMS,\n",
    "    PATH_SLIIDERS,\n",
    "    PATH_SLR_INT,\n",
    "    PATH_SLR_IS_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea621f90-f33e-4ebd-99c9-2e6a92786c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slr_0_year = pd.read_json(PATH_PARAMS).loc[\"slr_0_year\", \"values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea72758-ac27-49ea-9cc1-11a1ca4e9bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /data/raw/slr/ar6/regional/total_ssp245_low_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/total_ssp585_low_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/total_ssp126_low_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/total_ssp370_medium_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/total_ssp245_medium_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/total_ssp119_medium_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/total_ssp126_medium_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/total_ssp585_medium_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/verticallandmotion_ssp585_low_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/verticallandmotion_ssp245_low_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/verticallandmotion_ssp126_low_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/verticallandmotion_ssp245_medium_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/verticallandmotion_ssp370_medium_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/verticallandmotion_ssp126_medium_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/verticallandmotion_ssp119_medium_confidence_values.nc...\n",
      "Processing /data/raw/slr/ar6/regional/verticallandmotion_ssp585_medium_confidence_values.nc...\n"
     ]
    }
   ],
   "source": [
    "all_ds = []\n",
    "global_ds = []\n",
    "global_ssps = []\n",
    "\n",
    "\n",
    "def open_and_convert(ds_path):\n",
    "    out = xr.open_dataset(ds_path)\n",
    "\n",
    "    # for some reason the VLM dataset has an entry for 2005 that is all 0s, while other\n",
    "    # datasets just don't have 2005 b/c it is the assumed baseline\n",
    "    if out.years[0].item() == slr_0_year:\n",
    "        assert (out.sea_level_change.sel(years=slr_0_year) == 0).all()\n",
    "        out = out.isel(years=slice(1, None))\n",
    "\n",
    "    out[\"sea_level_change\"] = (\n",
    "        out.sea_level_change.pint.quantify().pint.to(\"meters\").pint.dequantify()\n",
    "    )\n",
    "\n",
    "    # add in the 2019 year so that we enforce pulses only happen in 2020\n",
    "    this = out.sea_level_change\n",
    "    init_years = this.years[0].item() - slr_0_year\n",
    "    init_year = (\n",
    "        (init_years - 1)\n",
    "        / init_years\n",
    "        * this.isel(years=0, drop=True).expand_dims(years=[out.years[0].item() - 1])\n",
    "    )\n",
    "    out = out.drop_dims(\"years\").assign(\n",
    "        sea_level_change=xr.concat((init_year, this), dim=\"years\")\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "for kind in [\"total\", \"verticallandmotion\"]:\n",
    "    ds = []\n",
    "    this_ssps = []\n",
    "    for conf in [\"low\", \"medium\"]:\n",
    "        for ds_path in (DIR_SLR_AR6_RAW / \"regional\").glob(f\"{kind}_*_{conf}_*\"):\n",
    "            print(f\"Processing {ds_path}...\")\n",
    "            this_ssp = ds_path.name.split(\"_\")[1]\n",
    "            ssp_conf = f\"{this_ssp}_{conf}\"\n",
    "            ds.append(open_and_convert(ds_path))\n",
    "            this_ssps.append(ssp_conf)\n",
    "            if kind == \"total\":\n",
    "                global_ds.append(\n",
    "                    open_and_convert(DIR_SLR_AR6_RAW / \"global\" / ds_path.name)\n",
    "                )\n",
    "                global_ssps.append(ssp_conf)\n",
    "    all_ds.append(\n",
    "        xr.concat(ds, pd.Index(this_ssps, name=\"scenario\"), data_vars=\"different\")\n",
    "    )\n",
    "\n",
    "global_ds = (\n",
    "    xr.concat(global_ds, pd.Index(global_ssps, name=\"scenario\"), data_vars=\"different\")\n",
    "    .squeeze(drop=True)\n",
    "    .drop_vars([\"lon\", \"lat\"])\n",
    "    .sea_level_change\n",
    ")\n",
    "\n",
    "# handle floating point matching errors on the quantile dimension\n",
    "global_ds[\"quantiles\"] = all_ds[0].quantiles\n",
    "all_ds[1][\"quantiles\"] = all_ds[0].quantiles\n",
    "\n",
    "all_ds = xr.Dataset(\n",
    "    {\n",
    "        \"lsl_msl05\": all_ds[0].sea_level_change,\n",
    "        \"lsl_ncc_msl05\": all_ds[1].sea_level_change,\n",
    "        \"gsl_msl05\": global_ds,\n",
    "        \"lon\": all_ds[1].lon,\n",
    "        \"lat\": all_ds[0].lat,\n",
    "    }\n",
    ")\n",
    "\n",
    "# drop locations with NaN values in the time period we're interested in\n",
    "valid = (\n",
    "    all_ds[[\"lsl_msl05\", \"lsl_ncc_msl05\"]]\n",
    "    .sel(years=slice(2100))\n",
    "    .notnull()\n",
    "    .all([\"scenario\", \"quantiles\", \"years\"])\n",
    "    .to_array(\"tmp\")\n",
    "    .all(\"tmp\")\n",
    ")\n",
    "all_ds = all_ds.sel(locations=valid)\n",
    "\n",
    "all_ds = all_ds.rename(\n",
    "    {\"years\": \"year\", \"quantiles\": \"quantile\", \"locations\": \"site_id\"}\n",
    ")\n",
    "\n",
    "# we generally allow +180 but not -180\n",
    "all_ds[\"lon\"] = all_ds.lon.where(all_ds.lon != -180, 180)\n",
    "\n",
    "# ensure no locations have missing values\n",
    "assert all_ds.sel(year=slice(2100)).notnull().all().to_array().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6751079-9920-4c9a-8013-7d58da1bb7ba",
   "metadata": {},
   "source": [
    "## Add on the pulse from greenland and antarctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef494eb-75ed-4d52-b99d-18cc67cd3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliiders = (\n",
    "    xr.open_zarr(str(PATH_SLIIDERS))\n",
    "    .load()\n",
    "    .swap_dims(seg_adm=\"seg\")\n",
    "    .drop_duplicates(\"seg\")\n",
    ")\n",
    "\n",
    "slrs = get_nearest_slrs(\n",
    "    all_ds,\n",
    "    sliiders[[\"seg_lon\", \"seg_lat\"]].reset_coords(drop=True).to_dataframe(),\n",
    ")\n",
    "\n",
    "adders = (\n",
    "    pd.read_parquet(PATH_SLR_IS_RAW)\n",
    "    .loc[sliiders.seg.to_series().index]\n",
    "    .assign(constant=0.01)\n",
    "    .to_xarray()\n",
    "    .to_array(\"sheet\")\n",
    ")\n",
    "# add in GSL\n",
    "adders = xr.Dataset(\n",
    "    {\"lsl_msl05\": adders, \"gsl_msl05\": xr.ones_like(adders.isel(seg=0, drop=True))}\n",
    ")\n",
    "\n",
    "seg_ds = all_ds.sel(site_id=slrs.to_xarray())\n",
    "del all_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b554612-0a30-4c5c-a354-c09343d6e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsl = seg_ds[[\"lsl_msl05\", \"gsl_msl05\"]].isel(year=slice(1, None)) + adders\n",
    "newsl = (\n",
    "    xr.concat(\n",
    "        (\n",
    "            seg_ds[[\"lsl_msl05\", \"gsl_msl05\"]]\n",
    "            .isel(year=[0])\n",
    "            .expand_dims(sheet=adders.sheet),\n",
    "            newsl,\n",
    "        ),\n",
    "        dim=\"year\",\n",
    "    )\n",
    "    .rename(scenario=\"tmp\")\n",
    "    .stack(tmp2=[\"tmp\", \"sheet\"])\n",
    ")\n",
    "newsl[\"scenario\"] = newsl.tmp + \"_\" + newsl.sheet\n",
    "newsl = newsl.set_index(tmp2=\"scenario\").reset_coords(drop=True).rename(tmp2=\"scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea0c2701-083b-4fca-961c-5a939b073998",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scens_added = xr.concat(\n",
    "    (\n",
    "        seg_ds[[\"lsl_msl05\", \"gsl_msl05\"]],\n",
    "        newsl,\n",
    "    ),\n",
    "    dim=\"scenario\",\n",
    ")\n",
    "\n",
    "out = (\n",
    "    xr.merge((seg_ds.drop_dims(\"scenario\"), new_scens_added))\n",
    "    .swap_dims(seg=\"site_id\")\n",
    "    .drop_vars(\"seg\")\n",
    "    .drop_duplicates(\"site_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a7c34-ee76-41f5-9e4d-6593201ff96a",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba5b3f7-23ac-4857-9ce0-f74558cc653b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7fd0200a73c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for v in out.variables:\n",
    "    out[v].encoding.clear()\n",
    "\n",
    "out.chunk({\"site_id\": 100}).to_zarr(str(PATH_SLR_INT), mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
